{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd5567a",
   "metadata": {},
   "source": [
    "# Bandits and gridworlds\n",
    "\n",
    "Franz Wurm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f321a",
   "metadata": {},
   "source": [
    "This is the code for the practical session in the course \"2122-S2 Reinforcement Learning & Decision-Making: Computational & Neural Mechanisms\" [(link to Brightspace)](https://brightspace.universiteitleiden.nl/d2l/home/134708)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f42bff",
   "metadata": {},
   "source": [
    "**Useful references**\n",
    "- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce7d02",
   "metadata": {},
   "source": [
    "## Goal of this practical\n",
    "\n",
    "In this practical, you are going to implement a reinforcement learning algorithm for two standard problems.\n",
    "\n",
    "In the [bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit), the agent is faced with a choice between two options. Named after slot machines in a casino, our two-armed version helps to demonstrate the the two core principles that allow the agent to achieve his goal of reward maximzation. \n",
    "\n",
    "Afterwards, we will also investigate reinforcement learning in a more realistic context. In the gridworld problem, the agent is again faced with choices between different options. However, in contrast to the bandit task, choices are sequential, feedback is scarce and the state space big. This poses a bigger problem for learning in the gridworld.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c5d65",
   "metadata": {},
   "source": [
    "## The bandit problem\n",
    "\n",
    "As discussed in the lecture, the two principles of reinforcement learning are\n",
    "- the learning rule: updating of expectations based on observations \n",
    "- decision rule: taking actions based on expectations\n",
    "\n",
    "### Decision rule\n",
    "\n",
    "The decision rule (or policy) defines the agent's way of behaving at a given time or state. Put differently, the policy is a mapping between states and actions and it corresponds to what psychologists sometimes call a stimulus-response association.\n",
    "\n",
    "During the lecture, we introduced a few different decision rules and of course the list in not exhaustive.\n",
    "- random method\n",
    "- greedy method\n",
    "- e-greedy method\n",
    "- softmax method\n",
    "\n",
    "Let's have a closer look at the softmax rule, as this is maybe the most widely used decision rule in the neuroscientific literature.\n",
    "\n",
    "$\\LARGE p(a)= \\frac{e ^{(\\beta * Q(a))}} {\\sum \\limits _{a'} e ^{(\\beta * Q(a'))}}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78e4336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = function(Q,beta) {\n",
    "  #needs completion\n",
    "  return(p)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e130f1",
   "metadata": {},
   "source": [
    "Putting this softmax function to use, we need to make a few hypothetical assumptions for our bandit task.\n",
    "\n",
    "For example, let's assume our agent has played the slot machine twice, each arm one time. The left arm lead to a reward (1), whereas the right arm did not result in a reward (0). We can translate this experience into simplified expectations (Q values) for the next game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d817741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: [1] 1\n",
      "Expectations: [1] 1 0\n",
      "Probabilities: [1] 0.7310586 0.2689414\n"
     ]
    }
   ],
   "source": [
    "actions = c(\"left\",\"right\")\n",
    "Qdummy = c(1,0)\n",
    "beta = 1\n",
    "\n",
    "p = softmax(Qdummy,beta)\n",
    "\n",
    "cat('beta: ')\n",
    "print(beta)\n",
    "cat('Expectations: ')\n",
    "print(Qdummy)\n",
    "cat('Probabilities: ')\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49748e43",
   "metadata": {},
   "source": [
    "We can see, that the dummy expectations translate into action probabilites in a straightforward way. The highest value has the highest action probability.\n",
    "\n",
    "**Question**: So what is the role of beta in this softmax function?\n",
    "\n",
    "You can write your answers below and discuss it with your colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d572524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the role of beta in the softmax function?\n",
    "# (you can change this code block to markdown)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e85300",
   "metadata": {},
   "source": [
    "Once you have answered the above question, you can play around with the beta value and observe the changes!\n",
    "\n",
    "You will see, that beta (or 'inverse temperature') affects the so-called [gain](https://en.wikipedia.org/wiki/Gain_(electronics)). The higher the gain, the more pronounced the differences in action values get translated into action probabilities.\n",
    "\n",
    "We can plot this in a systematic way.\n",
    "Try to understand what is plotted on the x and y axis, and how the different betas relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da828059",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(NA, ylim = c(0, 1), xlim = c(-2, 2), main = \"Softmax for 2 options\",\n",
    "     xlab = \"Q(A)-Q(B)\",\n",
    "     ylab = \"p(A)\") #better view over the lines\n",
    "x = seq(-2,2,0.05)\n",
    "betas = c(0,1,2,5,10)\n",
    "mypal <- colorRampPalette( c( \"red\", \"green\", \"blue\") )( length(betas) )\n",
    "for (i in 1:length(betas)) {\n",
    "  y = integer(length(x))\n",
    "  for (j in 1:length(x)){\n",
    "    p = softmax(Qdummy*x[j],betas[i])\n",
    "    y[j] = p[1]\n",
    "  }\n",
    "  lines(x,y, type = \"l\", col = mypal[i], , lwd=1.5) #thickness set to be higher\n",
    "}\n",
    "legend(\"topleft\", legend=as.character(betas), lwd = 1,col=mypal,title='betas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac0b29",
   "metadata": {},
   "source": [
    "As a next step, we finally want to introduce some action.\n",
    "\n",
    "Before we can make a decision, we need to define a reward and action function.\n",
    "\n",
    "- In the reward function, we specify the consequences of each possible actions.\n",
    "- In the action function, we simply determine the action taken based on the probabilities from the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9febefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "getReward_bandit = function(action) {\n",
    "    luck = runif(1)\n",
    "    if (action == \"right\") {\n",
    "       if (luck > 0.3) {\n",
    "           reward = 1\n",
    "       }  else {\n",
    "           reward = 0\n",
    "       }\n",
    "    } else if (action == \"left\") {\n",
    "       if (luck > 0.7) {\n",
    "           reward = 1\n",
    "       }  else {\n",
    "           reward = 0\n",
    "       } \n",
    "    }\n",
    "    outcome = list(\"luck\" = luck,\"reward\" = reward)\n",
    "    return(outcome)\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ecc1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "takeAction = function(values,options) {\n",
    "    if (length(values) == length(options)) {\n",
    "        p = softmax(values,beta)\n",
    "        action = sample(options, size = 1, prob = p)\n",
    "    } else {\n",
    "        print('Values dont match options!')\n",
    "    }\n",
    "    return(action)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cebd0a",
   "metadata": {},
   "source": [
    "We can now test our code and perform some actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = takeAction(Qdummy,actions)\n",
    "reward = getReward_bandit(action)\n",
    "\n",
    "print(action)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7dc535",
   "metadata": {},
   "source": [
    "You see that besides the action and the resulting reward I have also introduced a variable called \"luck\". While this is not a necessary variable, it helps us contrast a random agent from a learning agent.\n",
    "\n",
    "First, let's have a look at a random decision maker.\n",
    "\n",
    "As can be seen in the figure above, random action selection is a special case of the softmax method. If beta is set to zero, any existing difference between action values get egalized (remember: zero times something will always be zero...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b06be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sequential actions of a naive agent\n",
    "actions = c(\"left\",\"right\")\n",
    "nTrial = 1000\n",
    "\n",
    "Q = c(0.5,0.5)\n",
    "print(\"initial values\")\n",
    "print(Q)\n",
    "\n",
    "beta = 0\n",
    "\n",
    "luck = array(NaN,nTrial)\n",
    "reward = array(NaN,nTrial)\n",
    "for (iT in 1:nTrial) {\n",
    "    action = takeAction(Q,actions)\n",
    "    outcome = getReward_bandit(action)\n",
    "    luck[iT] = outcome$luck\n",
    "    reward[iT] = outcome$reward\n",
    "}\n",
    "print(paste(\"total luck: \",sum(luck)))\n",
    "print(paste(\"total reward: \",sum(reward)))\n",
    "\n",
    "print(\"final values\")\n",
    "print(Q)\n",
    "\n",
    "boxplot(luck, main=\"luck vs reward\")\n",
    "points(mean(reward), col=\"red\", pch=\"+\", cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609bba21",
   "metadata": {},
   "source": [
    "The boxplot shows the range of the agent's luck. In line with out definition, it is evenly spaced between 0 and 1, no outliers. \n",
    "\n",
    "In red you see the mean reward.\n",
    "\n",
    "As expected for a random agent, median luck and average reward are positively correlated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334aaa57",
   "metadata": {},
   "source": [
    "### Learning rule\n",
    "\n",
    "In a next step, we introduce the learning rule. This rule should help our agent to build up expectations about its environment, thereby leaving behind its dependence on luck.\n",
    "\n",
    "Let's specify the learning rule in accordance with the simplified TD formula introduced in the lecture.\n",
    "\n",
    "$\\LARGE Q_{new}(a) = Q_{old}(a) + \\alpha * PE$,\n",
    "\n",
    "    where $\\alpha$ is the learning rate,\n",
    "    and PE is the prediction error.\n",
    "    \n",
    "The prediction error is calculated as the difference between obtained and expected reward\n",
    "    \n",
    "$\\LARGE PE = R - Q_{old}(a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4210dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TDlearn_bandit = function(values,reward,action) {\n",
    "    #needs completion\n",
    "    return(values)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ccece",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = c(0.5,0.5)\n",
    "print(\"initial values\")\n",
    "print(Q)\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "luck = array(NaN,nTrial)\n",
    "reward = array(NaN,nTrial)\n",
    "for (iT in 1:nTrial) {\n",
    "    action = takeAction(Q,actions)   \n",
    "    outcome = getReward_bandit(action)    \n",
    "    luck[iT] = outcome$luck\n",
    "    reward[iT] = outcome$reward\n",
    "    \n",
    "    #print(Q)\n",
    "    #print(action)\n",
    "    Q = TDlearn_bandit(Q,outcome$reward,which(actions==action))\n",
    "}\n",
    "print(paste(\"total luck: \",sum(luck)))\n",
    "print(paste(\"total reward: \",sum(reward)))\n",
    "\n",
    "print(\"final values\")\n",
    "print(Q)\n",
    "boxplot(luck, main=\"luck vs reward\")\n",
    "points(mean(reward), col=\"red\", pch=\"+\", cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ffc46",
   "metadata": {},
   "source": [
    "Again, the boxplot shows the range of experienced luck and the mean reward.\n",
    "\n",
    "**Question**: Contrary to our expectations, the average reward is not significantly different from chance performance (see random agent above). Looking at the final expectations we can clearly see, that the environmental differences between options have been learned (right actions  have a higher reward probability than left action). So how come that this \"knowledge\" is not reflected in the decision-making and average reward?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why is performance for our learning agent not different from our random agent?\n",
    "# (you can change this code block to markdown)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2384c7a",
   "metadata": {},
   "source": [
    "**Answer**: If we take a step back and have a look at the decision rule, we can see that the inverse temperature parameter is still fixed at 0. This means that there is perfect negative gain, and therefore differences in expectations get egalized during action selection.\n",
    "\n",
    "Add a beta parameter, set it to 10 (positive gain) and have a look at the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6cb4b",
   "metadata": {},
   "source": [
    "\n",
    "**Excercise 1.** So far, when comparing our random and learning agent, the term \"not significantly different from chance performance\" is purely descriptive. In order to be able to make a stronger claim or inference, we could simulate multiple agents and compare our decision rules on the population level. Simulate multiple agents for each method by adding another loop. Store and plot the relevant metrics to show that learning agent truely outperform random agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f43b770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## code block for exercise 1 #\n",
    "\n",
    "\n",
    "\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2392db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list = ls()) #clear the workspace before we continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52476bc",
   "metadata": {},
   "source": [
    "### The grid world problem\n",
    "\n",
    "*Grid worlds* are simplified respresentations of the environment, that are often used for navigation simulation. As is implied by the name, grid worlds break down the environment into a grid, similar to a chess board. For the scope of this course we are going to stick with a very basic 4x4 gridworld.\n",
    "\n",
    "Before we dive further into the code, a bit more background information on gridworlds. \n",
    "\n",
    "Gridworlds are so-called [Markov Decision Processes](https://en.wikipedia.org/wiki/Markov_decision_process). In contrast to the previous bandit task, gridworlds usually are multi-step problems, meaning that actions executed in one state cannot only result in reward, but also affect the upcoming state. This necessitates the agent to not only consider the immediate reward but also the expected cumulative reward. \n",
    "\n",
    "Our agent always starts in the same start state (s_0, top left of board). From there, it will take *steps*, that gradually move him across the board. Movement is restricted to the cardinal directions (up, down, right, left). Reward is located in the terminal state (s_terminal, bottom right of board). Upon arrival at the site of reward, the agent receives the reward (associated with a positive value) and will be returned to the initial state, so the whole procedure can start again. The (time)steps between start and terminal state are regarded as a *run* (or *episode*). Start state, reward and terminal state do NOT change between runs. Thus, our gridworld environment is stable.\n",
    "\n",
    "For this grid world example, we will implement the Q learning rule, which is defined as\n",
    "\n",
    "$\\LARGE Q_{new}(s,a) = Q_{old}(s,a) + \\alpha * (R + \\gamma max_{a} Q_{old}(s',a) - Q_{old}(s,a))$,\n",
    "\n",
    "    where $\\alpha$ is the learning rate,\n",
    "    $\\gamma$ is the discounting factor,\n",
    "    and s' is the next state\n",
    "\n",
    "Below, we define the most basic details for our gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585b1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied function from bandit\n",
    "takeAction = function(values,options) {\n",
    "    if (length(values) == length(options)) {\n",
    "        p = softmax(values,beta)\n",
    "        action = sample(options, size = 1, prob = p)\n",
    "    } else {\n",
    "        print('Values dont match options!')\n",
    "    }\n",
    "    return(action)\n",
    "}\n",
    "\n",
    "# copied function from bandit\n",
    "softmax = function(Q,beta) {\n",
    "  p = exp(beta * Q) / sum(exp(beta * Q))\n",
    "  return(p)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 4\n",
    "\n",
    "s_0 <- 0 # start at initial state\n",
    "s_terminal <- (grid_size^2)-1 # goal state\n",
    "actions <- c(\"left\", \"up\", \"right\", \"down\")\n",
    "\n",
    "# assign numbers to each state in the gridworld\n",
    "states = matrix(, nrow = grid_size, ncol = grid_size)\n",
    "iZ = 0\n",
    "for (iX in 1:grid_size) {\n",
    "  for (iY in 1:grid_size) {\n",
    "    states[iY, iX] = iZ\n",
    "    iZ = iZ + 1\n",
    "  }\n",
    "}\n",
    "\n",
    "print(paste(\"state state: \",s_0))\n",
    "print(paste(\"final state: \",s_terminal))\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffcbc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "getReward_grid <- function(state) {\n",
    "  if (state == s_terminal) {\n",
    "    reward <- 100\n",
    "  } else {\n",
    "    reward <- -1\n",
    "  }\n",
    "  return(reward)\n",
    "}\n",
    "\n",
    "makeMove <- function(sin, ain) {\n",
    "  sout = sin \n",
    "  coords = which(states == sin, arr.ind = TRUE)  \n",
    "  if (ain == \"down\")\n",
    "    coords[1] <- coords[1] + 1\n",
    "  if (ain == \"up\")\n",
    "    coords[1] <- coords[1] - 1\n",
    "  if (ain == \"right\")\n",
    "    coords[2] <- coords[2] + 1\n",
    "  if (ain == \"left\")\n",
    "    coords[2] <- coords[2] - 1\n",
    "  \n",
    "  if (coords[1] < 1)\n",
    "    coords[1] = 1\n",
    "  if (coords[1] > length(states[, 1]))\n",
    "    coords[1] = length(states[, 1])\n",
    "  if (coords[2] < 1)\n",
    "    coords[2] = 1\n",
    "  if (coords[2] > length(states[1, ]))\n",
    "    coords[2] = length(states[1, ])\n",
    "  \n",
    "  sout = states[coords]  \n",
    "  return(sout)\n",
    "}\n",
    "\n",
    "TDlearn_grid = function(values,reward,action,state,nextstate) {\n",
    "    #needs completion (see gridworld learning rule!)\n",
    "    return(values)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c37ab",
   "metadata": {},
   "source": [
    "Now that we have defined the most crucial parts for our grid world, we can put everything together.\n",
    "\n",
    "For convenience, I have already added multiple agents, as this will allow us to draw more precise conclusions from the simulation data.\n",
    "\n",
    "For the agent's parameters I have implemented very basic values. You can have a look how changing those parameters affects the performance in the grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5082ec97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nAgent = 20\n",
    "nRun = 100\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 1\n",
    "\n",
    "movecounter = matrix(0, nrow = nRun, ncol = nAgent)\n",
    "Qall = array(0,c(grid_size^2, length(actions), nAgent))\n",
    "\n",
    "for (iAgent in 1:nAgent) {\n",
    "    \n",
    "#     print(paste(\"agent \",iAgent))\n",
    "    Q <- array(0, c(grid_size^2, length(actions)))\n",
    "\n",
    "    for (iRun in 1:nRun) {\n",
    "    \n",
    "#         if (iRun %% 20 == 0) {\n",
    "#           print(paste(\"   trial \",iRun))\n",
    "#         }\n",
    "    \n",
    "        state <- s_0 # set cursor to initial state\n",
    "        sidx = which(states == state)\n",
    "    \n",
    "        moves = 0   \n",
    "        while (state != s_terminal) {\n",
    "        \n",
    "            action = takeAction(Q[sidx,],actions)\n",
    "            next_state <- makeMove(state, action)\n",
    "            reward <- getReward_grid(next_state)\n",
    "        \n",
    "            aidx = which(actions == action)\n",
    "            s2idx = which(states == next_state)\n",
    "            Q = TDlearn_grid(Q,reward,aidx,sidx,s2idx)\n",
    "        \n",
    "            response <- (list(state = next_state, reward = reward))\n",
    "\n",
    "      \n",
    "#             if ((iRun == 1) & (next_state == s_terminal))\t{\n",
    "#                print(paste(\"trial \",iRun))\n",
    "#                print(Q) # what does q-look like after first successful iteration (i==1)\n",
    "#             }\n",
    "#             if ((iRun == nRun) & (next_state == s_terminal))\t{\n",
    "#                print(paste(\"trial \",iRun))\n",
    "#                print(Q) # what does q-look like after first successful iteration (i==1)\n",
    "#             }\n",
    "          \n",
    "            state <- response$state # move to next state\n",
    "            sidx = which(states == state)\n",
    "      \n",
    "            moves = moves + 1\n",
    "        }\n",
    "        Qall[,,iAgent] = Q\n",
    "        movecounter[iRun, iAgent] = moves\n",
    "    }\n",
    "}\n",
    "#print(movecounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea8542",
   "metadata": {},
   "source": [
    "In order to better understand the agents' behavior in the gridworld we can again make use of the `print()` and `plot()` function.\n",
    "\n",
    "**Question**: As a first step, lets have a look at the performance of our agent. Do you have any hypothesis about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is your hypothesis?\n",
    "# (you can change this code block to markdown)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4a354",
   "metadata": {},
   "source": [
    "**Answer**: Performance for learning agents should improve over time. Therefore we should expect to find improvements in performance for our grid world agent. Performance can be measured in multiple different ways. For our grid world example, we could define 2 measures of preformance. The first measure is already plotted below: the number of steps it takes the agents to get from start state to terminal state. While the agent is aimlessly roaming the gridworld in the beginning, it quickly picks up the optimal path to the goal.\n",
    "\n",
    "**Excercise 2** Define a second measure of performance, implement it in the code and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d62ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting performance\n",
    "plot(1:nRun, apply(movecounter, c(1), mean), xlab = \"Runs\", ylab = \"Steps\", main=\"Performance\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d87b67",
   "metadata": {},
   "source": [
    "Finally, we can also have a look at the policy of our agents.\n",
    "\n",
    "Plotting the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions2plot <- c(\"left \", \" up  \", \"right\", \"down \")\n",
    "\n",
    "\n",
    "a = apply(Qall, c(1,2), mean)\n",
    "b = apply(a, c(1), which.max)\n",
    "dim(b) = c(grid_size,grid_size)\n",
    "\n",
    "#print(a)\n",
    "\n",
    "print(\"policy for recent agent\")\n",
    "for (i in 1:grid_size) {\n",
    "  cat('|')\n",
    "  for (j in 1:grid_size) {\n",
    "    if (states[i,j]==s_0) {\n",
    "      cat(paste('*',actions2plot[b[i,j]],'*|'))\n",
    "    } else if (states[i,j]==s_terminal) {\n",
    "      cat(paste('+',actions2plot[b[i,j]],'+|'))\n",
    "    } else {\n",
    "      cat(paste(' ',actions2plot[b[i,j]],' |'))\n",
    "    }\n",
    "  }\n",
    "  print('')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51ec39",
   "metadata": {},
   "source": [
    "**Excercise 3.** So far, we have only investigated the softmax and random method for action selection, with random action selection being a special case of softmax action selection. Try to implement alternative methods, such as greedy or e-greedy and have a look how performance changes. \n",
    "\n",
    "**Excercise 4.** The current grid world is perfectly deterministic. Actions reliably lead to the same outcome. Reward is always hidden in the same location. Recall that for our bandit example reward delivery was probabilistic. Can you implement some randomness in the grid world as well? Think about a few possible options. How could you implement those and what would be the consequences for agents' learning and decision-making?\n",
    "\n",
    "**Excercise 5.** (optional) Although we have only discussed different decision rules at this point, we could also think about different learning rules. A prominent example is already described in Sutton & Barto (2018, p132). In their variant of the grid world, learning for the different learning rules translates into very distinct behavioral patterns. It nicely illustrates how small changes in the algorithm can have quite strong effects on decision making. Have a look at the example. Can you construct a cliff world and replicate the findings reported by Sutton & Barto?      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
